---
title: "Introduction to the lineSampler Package"
author: "Drew Schmidt"
date: "`r Sys.Date()`"
output:
  html_document:
    includes:
      in_header: include/header.html
    fig_caption: true
    toc: true
    number_sections: true
    css: include/custom.css
    highlight: kate
bibliography: include/lineSampler.bib
csl: "include/ieee.csl"
vignette: >
  %\VignetteIndexEntry{Introduction to the lineSampler Package}
  %\VignetteEngine{knitr::rmarkdown}
---


# Introduction

**lineSampler** [[@lineSampler]] is a simple R package for reading random subsamples of flat text files by line in a reasonably efficient manner.  This is useful if you have a very large file (too big to comfortably handle in memory) and you want to prototype something on an unbiased subset.

Basically, each reader works by sampling as the input file is scanned and randomly choosing whether or not to dump the current line to an external temporary file.  This temporary file is then read back into R.  For (aggressive)  downsampling, this is a very effective strategy; for resampling, you are much better off reading the full dataset into memory.
 
This package, including the underlying C library, is licensed under the permissive 2-clause BSD license.  The original idea was inspired by Eduardo Arino de la Rubia's **fast_sample** [[@fast_sample]].


## Installation
You can install the stable version from CRAN using the usual `install.packages()`:

```r
install.packages("lineSampler")
```

The development version is maintained on GitHub, and can easily be installed by any of the packages that offer installations from GitHub:

```r
### Pick your preference
devtools::install_github("wrathematics/lineSampler")
ghit::install_github("wrathematics/lineSampler")
remotes::install_github("wrathematics/lineSampler")
```





# Using the Package

At its most basic, you can use just the `sample_csv()` for csv files.  Say you want to read in 0.1% of the data.  Then you would call:

```r
sample_csv(file, 0.001)
```

and `sample_lines()` for reading unstructured lines (as `readLines()`):

```r
sample_lines(file, 0.001)
```

By default, each of these file samplers will sample proportionally.  This is done in a single pass of the input file.  However, you can also use an "exact" sampler, which uses a reservoir sampler to draw an exact number of lines at random.  Say you wanted to read in 1000 lines of a csv.  Then you would call:

```r
sample_csv(file, 1000, method="exact")
```

This makes two passes through the file.  Implementation details are discussed in a later section of this vignette.


## Other Readers
To keep external dependencies to a minimum, only the base R `read.csv()` and `readLines()` are directly supported.  If you wish to use other readers like `fread()` from **data.table** [[@datatable]] or `read_csv()` from **readr** [[@readr]], you can pass one of these as the argument `reader` to the \code{sample_csv()} function.  Examples of this can be found in the Benchmarks section below.

Note that if the downsampling was sufficiently aggressive, you will probably not notice any performance improvement using these better readers over `read.csv()`.





# Benchmarks

## Benchmark Setup
All benchmarks were performed using:

* A Core i5-2500K CPU @ 3.30GHz with a platter HDD
* R 3.3.1
* Package versions:
    - 0.4-0 of **lineSampler**
    - 1.0.0 of **readr**
    - 1.9.6 of **data.table**

Each timing shown is the result of running each test twice and taking the best (lowest) time.  This is to hopefully ensure that there's no file caching or bad RNG behavior making one reader look unreasonably slow compared to the others.  For each of the tests, there was no major difference in run times between the two runs.

The file `big.csv` referred to below was generated with the included `makebig` script found in `inst/benchmarks/` tree of the source for this package..


## The Benchmarks
The package should perform very well, provided the total number of lines sampled is fairly small.  For example, consider the file:

```r
file <- "/tmp/big.csv"

library(memuse)
Sys.filesize(file)
## 976.868 MiB

library(lineSampler)
wc_l(file)
## file:   /tmp/big.csv 
## Lines: 16000001 
```

We can read in approximately 0.1% of the input file quite quickly:

```r
system.time(small <- sample_csv(file, .001))
##  user  system elapsed 
## 1.044   0.128   1.172 
```

Compare this to the time spent reading the entire file:

```r
system.time(full <- read.csv(file))
##    user  system elapsed 
##  72.988   0.500  73.491

system.time(full <- read_csv(file, progress=FALSE))
##   user  system elapsed 
## 27.316   0.276  27.595 

system.time(full <- fread(file))
##    user  system elapsed 
##  12.328   0.100  12.430 
```

Note the difference in memory usage:

```r
dim(small)
## 16036     6
memuse(small)
## 568.477 KiB

dim(full)
## [1] 16000000        6
memuse(full)
## 549.321 MiB
```

Also notice that the in-memory file size of the full file times the proportion of lines read in is roughly the size of the downsampled file:

```r
memuse(full) * .001
## 562.505 KiB
```

Obviously the **lineSampler** strategy is valuable only for aggressive downsampling, and not resampling.


## Combining lineSampler with Other Readers
The bulk of the time spent in `sample_csv()` is not in the csv reading/parsing itself, but rather in the downsampling.  Consider:

```r
# the default reader
system.time(small <- sample_csv(file, .001, reader=read.csv))
##   user  system elapsed 
##  1.004   0.172   1.180 

# readr
system.time(small <- sample_csv(file, .001, reader=read_csv))
##   user  system elapsed 
##  1.024   0.108   1.130 

# data.table
system.time(small <- sample_csv(file, .001, reader=fread))
##   user  system elapsed 
##  0.928   0.184   1.112 
```

The read times do indeed go down, but not incredibly; the difference is probably unnoticeable to a human.  For larger reads, the times are more significant.  We can quickly see this by reading in 1% of the data:

```r
system.time(small <- sample_csv(file, .01, reader=read.csv))
##   user  system elapsed 
##  1.712   0.136   1.855 

system.time(small <- sample_csv(file, .01, reader=read_csv))
##   user  system elapsed 
##  1.236   0.160   1.397 

system.time(small <- sample_csv(file, .01, reader=fread))
##   user  system elapsed 
##  1.100   0.148   1.246 
```


## Exact Sampling
Recall that we can sample an exact number of lines using a reservoir sampler, rather than drawing lines proportionally.  We can enable this behavior by setting `method="exact"` in `sample_csv()`.

Before we were sampling about 16,000 and 160,000 lines (for `p=0.001` and `p=0.01` respectively), so we'll use that value for the exact sampler:

```r
system.time(small <- sample_csv(file, 16000, method="exact"))
##   user  system elapsed 
##  1.144   0.292   1.437 

system.time(small <- sample_csv(file, 160000, method="exact"))
##   user  system elapsed 
##  1.892   0.232   2.124 
```

In each case, the times are a bit slower, reflecting some extra work needed in concocting the reservoir (for example, getting line counts for the file).


## Conclusions and Summary
Based on the benchmarks, we find:

* If you want to very quickly read in a small amount of data, **lineSampler** is very effective.  The canonical use cases for this are if you:
    - want to take a quick, unbiased peek at the data
    - don't have enough memory to read in/operate on the full data set
* If your data isn't very big, you can probably comfortably read it in very quickly with **data.table**'s `fread()`.

Finally, if your csv is very large, say greater than 10GiB, I would strongly encourage you to choose another (preferably binary) format.  I have seen terabyte sized csv's, but it's not a good idea!





# Implementation Details

Here we describe the implementation details for the file sampling functions.  The other package export, `wc()`, has been aggressively optimized; but its implementation is not very interesting, so we do not belabor it here.  We will also only focus on the csv samplers, as in fact the raw text and csv schemes are almost identical.

The proportional sampling is handled by the function `LS_sample_prob()`, which samples lines at a given proportion.  As the input file is scanned line-by-line,  lines are randomly selected to be placed into a temporary file at the given proportion.  This requires only one pass through the file.  On the other hand, the exact sampling is handled by `LS_sample_exact()`.  Here we use a reservoir sampler to determine ahead of time which lines will be read, and then pass through the input file, dumping the pre-selected lines to the temporary file.  This requires two passes through the file, since we need to know the total number of lines.  In each case, after the downsampling takes place we read the temporary file into R using one of its csv readers.





# Legal

&copy; 2016 Drew Schmidt.

This manual is licensed under a [Creative Commons CC-BY license](http://creativecommons.org/licenses/by/4.0/).

This manual may be incorrect or out-of-date.  The authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.

<script language="JavaScript" src="include/headers.js"></script>
