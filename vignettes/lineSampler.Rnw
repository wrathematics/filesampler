%\VignetteIndexEntry{Introduction to the lineSampler Package}
\documentclass[]{article}


\input{./include/settings}


\mytitle{Introduction to the lineSampler Package}
\mysubtitle{}
\myversion{0.4-0}
\myauthor{
\centering
Drew Schmidt \\ 
\texttt{wrathematics@gmail.com} 
}


\begin{document}
\makefirstfew



\section{Introduction}\label{introduction}

\textbf{lineSampler}~\cite{lineSampler} is a simple R package for
reading random subsamples of flat text files by line in a reasonably
efficient manner. This is useful if you have a very large file (too big
to comfortably handle in memory) and you want to prototype something on
an unbiased subset.

Basically, each reader works by sampling as the input file is scanned
and randomly choosing whether or not to dump the current line to an
external temporary file. This temporary file is then read back into R.
For (aggressive) downsampling, this is a very effective strategy; for
resampling, you are much better off reading the full dataset into
memory.

This package, including the underlying C library, is licensed under the
permissive 2-clause BSD license. The original idea was inspired by
Eduardo Arino de la Rubia's \textbf{fast\_sample}
@fast\_sample.

\subsection{Installation}\label{installation}

You can install the stable version from CRAN using the usual
\texttt{install.packages()}:

\begin{lstlisting}[language=rr]
install.packages("lineSampler")
\end{lstlisting}

The development version is maintained on GitHub, and can easily be
installed by any of the packages that offer installations from GitHub:

\begin{lstlisting}[language=rr]
### Pick your preference
devtools::install_github("wrathematics/lineSampler")
ghit::install_github("wrathematics/lineSampler")
remotes::install_github("wrathematics/lineSampler")
\end{lstlisting}

\section{Using the Package}\label{using-the-package}

At its most basic, you can use just the \texttt{sample\_csv()} for csv
files. Say you want to read in 0.1\% of the data. Then you would call:

\begin{lstlisting}[language=rr]
sample_csv(file, 0.001)
\end{lstlisting}

and \texttt{sample\_lines()} for reading unstructured lines (as
\texttt{readLines()}):

\begin{lstlisting}[language=rr]
sample_lines(file, 0.001)
\end{lstlisting}

By default, each of these file samplers will sample proportionally. This
is done in a single pass of the input file. However, you can also use an
``exact'' sampler, which uses a reservoir sampler to draw an exact
number of lines at random. Say you wanted to read in 1000 lines of a
csv. Then you would call:

\begin{lstlisting}[language=rr]
sample_csv(file, 1000, method="exact")
\end{lstlisting}

This makes two passes through the file. Implementation details are
discussed in a later section of this vignette.

\subsection{Other Readers}\label{other-readers}

To keep external dependencies to a minimum, only the base R
\texttt{read.csv()} and \texttt{readLines()} are directly supported. If
you wish to use other readers like \texttt{fread()} from
\textbf{data.table}~\cite{datatable} or \texttt{read\_csv()} from
\textbf{readr}~\cite{readr}, you can pass one of these as the
argument \texttt{reader} to the \code{sample_csv()} function. Examples
of this can be found in the Benchmarks section below.

Note that if the downsampling was sufficiently aggressive, you will
probably not notice any performance improvement using these better
readers over \texttt{read.csv()}.



\section{Benchmarks}\label{benchmarks}

\subsection{Benchmark Setup}\label{benchmark-setup}

All benchmarks were performed using:

\begin{itemize}
\item
  A Core i5-2500K CPU @ 3.30GHz with a platter HDD
\item
  R 3.3.1
\item
  Package versions:

  \begin{itemize}
  \item
    0.4-0 of \textbf{lineSampler}
  \item
    1.0.0 of \textbf{readr}
  \item
    1.9.6 of \textbf{data.table}
  \end{itemize}
\end{itemize}

Each timing shown is the result of running each test twice and taking
the best (lowest) time. This is to hopefully ensure that there's no file
caching or bad RNG behavior making one reader look unreasonably slow
compared to the others. For each of the tests, there was no major
difference in run times between the two runs.

The file \texttt{big.csv} referred to below was generated with the
included \texttt{makebig} script found in \texttt{inst/benchmarks/} tree
of the source for this package..

\subsection{The Benchmarks}\label{the-benchmarks}

The package should perform very well, provided the total number of lines
sampled is fairly small. For example, consider the file:

\begin{lstlisting}[language=rr]
file <- "/tmp/big.csv"

library(memuse)
Sys.filesize(file)
## 976.868 MiB

library(lineSampler)
wc_l(file)
## file:   /tmp/big.csv 
## Lines: 16000001 
\end{lstlisting}

We can read in approximately 0.1\% of the input file quite quickly:

\begin{lstlisting}[language=rr]
system.time(small <- sample_csv(file, .001))
##  user  system elapsed 
## 1.044   0.128   1.172 
\end{lstlisting}

Compare this to the time spent reading the entire file:

\begin{lstlisting}[language=rr]
system.time(full <- read.csv(file))
##    user  system elapsed 
##  72.988   0.500  73.491

system.time(full <- read_csv(file, progress=FALSE))
##   user  system elapsed 
## 27.316   0.276  27.595 

system.time(full <- fread(file))
##    user  system elapsed 
##  12.328   0.100  12.430 
\end{lstlisting}

Note the difference in memory usage:

\begin{lstlisting}[language=rr]
dim(small)
## 16036     6
memuse(small)
## 568.477 KiB

dim(full)
## [1] 16000000        6
memuse(full)
## 549.321 MiB
\end{lstlisting}

Also notice that the in-memory file size of the full file times the
proportion of lines read in is roughly the size of the downsampled file:

\begin{lstlisting}[language=rr]
memuse(full) * .001
## 562.505 KiB
\end{lstlisting}

Obviously the \textbf{lineSampler} strategy is valuable only for
aggressive downsampling, and not resampling.

\subsection{Combining lineSampler with Other
Readers}\label{combining-linesampler-with-other-readers}

The bulk of the time spent in \texttt{sample\_csv()} is not in the csv
reading/parsing itself, but rather in the downsampling. Consider:

\begin{lstlisting}[language=rr]
# the default reader
system.time(small <- sample_csv(file, .001, reader=read.csv))
##   user  system elapsed 
##  1.004   0.172   1.180 

# readr
system.time(small <- sample_csv(file, .001, reader=read_csv))
##   user  system elapsed 
##  1.024   0.108   1.130 

# data.table
system.time(small <- sample_csv(file, .001, reader=fread))
##   user  system elapsed 
##  0.928   0.184   1.112 
\end{lstlisting}

The read times do indeed go down, but not incredibly; the difference is
probably unnoticeable to a human. For larger reads, the times are more
significant. We can quickly see this by reading in 1\% of the data:

\begin{lstlisting}[language=rr]
system.time(small <- sample_csv(file, .01, reader=read.csv))
##   user  system elapsed 
##  1.712   0.136   1.855 

system.time(small <- sample_csv(file, .01, reader=read_csv))
##   user  system elapsed 
##  1.236   0.160   1.397 

system.time(small <- sample_csv(file, .01, reader=fread))
##   user  system elapsed 
##  1.100   0.148   1.246 
\end{lstlisting}

\subsection{Exact Sampling}\label{exact-sampling}

Recall that we can sample an exact number of lines using a reservoir
sampler, rather than drawing lines proportionally. We can enable this
behavior by setting \texttt{method="exact"} in \texttt{sample\_csv()}.

Before we were sampling about 16,000 and 160,000 lines (for
\texttt{p=0.001} and \texttt{p=0.01} respectively), so we'll use that
value for the exact sampler:

\begin{lstlisting}[language=rr]
system.time(small <- sample_csv(file, 16000, method="exact"))
##   user  system elapsed 
##  1.144   0.292   1.437 

system.time(small <- sample_csv(file, 160000, method="exact"))
##   user  system elapsed 
##  1.892   0.232   2.124 
\end{lstlisting}

In each case, the times are a bit slower, reflecting some extra work
needed in concocting the reservoir (for example, getting line counts for
the file).

\subsection{Conclusions and Summary}\label{conclusions-and-summary}

Based on the benchmarks, we find:

\begin{itemize}
\item
  If you want to very quickly read in a small amount of data,
  \textbf{lineSampler} is very effective. The canonical use cases for
  this are if you:

  \begin{itemize}
  \item
    want to take a quick, unbiased peek at the data
  \item
    don't have enough memory to read in/operate on the full data set
  \end{itemize}
\item
  If your data isn't very big, you can probably comfortably read it in
  very quickly with \textbf{data.table}'s \texttt{fread()}.
\end{itemize}

Finally, if your csv is very large, say greater than 10GiB, I would
strongly encourage you to choose another (preferably binary) format. I
have seen terabyte sized csv's, but it's not a good idea!



\section{Implementation Details}\label{implementation-details}

Here we describe the implementation details for the file sampling
functions. The other package export, \texttt{wc()}, has been
aggressively optimized; but its implementation is not very interesting,
so we do not belabor it here. We will also only focus on the csv
samplers, as in fact the raw text and csv schemes are almost identical.

The proportional sampling is handled by the function
\texttt{LS\_sample\_prob()}, which samples lines at a given proportion.
As the input file is scanned line-by-line, lines are randomly selected
to be placed into a temporary file at the given proportion. This
requires only one pass through the file. On the other hand, the exact
sampling is handled by \texttt{LS\_sample\_exact()}. Here we use a
reservoir sampler to determine ahead of time which lines will be read,
and then pass through the input file, dumping the pre-selected lines to
the temporary file. This requires two passes through the file, since we
need to know the total number of lines. In each case, after the
downsampling takes place we read the temporary file into R using one of
its csv readers.



\addcontentsline{toc}{section}{References}
\bibliography{./include/lineSampler}
\bibliographystyle{plain}

\end{document}